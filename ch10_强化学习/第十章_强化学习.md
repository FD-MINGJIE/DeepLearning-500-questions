# 第十章 强化学习
## 10.1强化学习的主要特点？
其他许多机器学习算法中学习器都是学得怎样做，而RL是在尝试的过程中学习到在特定的情境下选择哪种行动可以得到最大的回报。在很多场景中，当前的行动不仅会影响当前的rewards，还会影响之后的状态和一系列的rewards。RL最重要的3个特定在于：
(1)	基本是以一种闭环的形式；
(2)	不会直接指示选择哪种行动（actions）；
(3)	一系列的actions和奖励信号（reward signals）都会影响之后较长的时间。 

### 1. 定义
强化学习是机器学习的一个重要分支，是多学科多领域交叉的一个产物，它的本质是解决 decision making 问题，即自动进行决策，并且可以做连续决策。
它主要包含四个元素，agent，环境状态，行动，奖励, 强化学习的目标就是获得最多的累计奖励。
我们列举几个形象的例子：
小孩想要走路，但在这之前，他需要先站起来，站起来之后还要保持平衡，接下来还要先迈出一条腿，是左腿还是右腿，迈出一步后还要迈出下一步。
小孩就是 agent，他试图通过采取行动（即行走）来操纵环境（行走的表面），并且从一个状态转变到另一个状态（即他走的每一步），当他完成任务的子任务（即走了几步）时，孩子得到奖励（给巧克力吃），并且当他不能走路时，就不会给巧克力。

![](./img/ch10/10-1.png)
 
上图中agent代表自身，如果是自动驾驶，agent就是车；如果你玩游戏它就是你当前控制的游戏角色，如马里奥，马里奥往前走时环境就一直在发生变化，有小怪物或者障碍物出现，它需要通过跳跃来进行躲避，就是要做action（如向前走和跳起的动作）；无人驾驶的action就是车左转、右转或刹车等等，它无时无刻都在与环境产生交互，action会反馈给环境，进而改变环境，如果自动驾驶的车行驶目标是100米，它向前开了10米，那环境就发生了变化，所以每次产生action都会导致环境改变，环境的改变会反馈给自身（agent），就是这样的一个循环；反馈又两种方式：1、做的好（reward）即正反馈，2、做得不好（punishment惩罚）即负反馈。Agent可能做得好，也可能做的不好，环境始终都会给它反馈，agent会尽量去做对自身有利的决策，通过反反复复这样的一个循环，agent会越来越做的好，就像孩子在成长过程中会逐渐明辨是非，这就是强化学习。
## 10.2强化学习应用实例
（1）Manufacturing

例如一家日本公司 Fanuc，工厂机器人在拿起一个物体时，会捕捉这个过程的视频，记住它每次操作的行动，操作成功还是失败了，积累经验，下一次可以更快更准地采取行动。

![](./img/ch10/10-2.png)

（2）Inventory Management

在库存管理中，因为库存量大，库存需求波动较大，库存补货速度缓慢等阻碍使得管理是个比较难的问题，可以通过建立强化学习算法来减少库存周转时间，提高空间利用率。

（3）Dynamic pricing

强化学习中的 Q-learning 可以用来处理动态定价问题。

（4）Customer Delivery

制造商在向各个客户运输时，想要在满足客户的所有需求的同时降低车队总成本。通过 multi-agents 系统和 Q-learning，可以降低时间，减少车辆数量。

（5）ECommerce Personalization

在电商中，也可以用强化学习算法来学习和分析顾客行为，定制产品和服务以满足客户的个性化需求。
（6）Ad Serving

例如算法 LinUCB （属于强化学习算法 bandit 的一种算法），会尝试投放更广范围的广告，尽管过去还没有被浏览很多，能够更好地估计真实的点击率。
再如双 11 推荐场景中，阿里巴巴使用了深度强化学习与自适应在线学习，通过持续机器学习和模型优化建立决策引擎，对海量用户行为以及百亿级商品特征进行实时分析，帮助每一个用户迅速发现宝贝，提高人和商品的配对效率。还有，利用强化学习将手机用户点击率提升了 10-20%。

（7）Financial Investment Decisions

例如这家公司 Pit.ai，应用强化学习来评价交易策略，可以帮助用户建立交易策略，并帮助他们实现其投资目标。

（8）Medical Industry

动态治疗方案（DTR）是医学研究的一个主题，是为了给患者找到有效的治疗方法。 例如癌症这种需要长期施药的治疗，强化学习算法可以将患者的各种临床指标作为输入 来制定治疗策略。
## 10.3强化学习和监督式学习、非监督式学习的区别
在机器学习中，我们比较熟知的是监督式学习，非监督学习，此外还有一个大类就是强化学习：
当前的机器学习算法可以分为3种：有监督的学习（Supervised Learning）、无监督的学习（Unsupervised Learning）和强化学习（Reinforcement Learning），结构图如下所示：

 ![](./img/ch10/10-3.png)

### 强化学习和监督式学习的区别：
监督式学习就好比你在学习的时候，有一个导师在旁边指点，他知道怎么是对的怎么是错的，但在很多实际问题中，例如 chess，go，这种有成千上万种组合方式的情况，不可能有一个导师知道所有可能的结果。

而这时，强化学习会在没有任何标签的情况下，通过先尝试做出一些行为得到一个结果，通过这个结果是对还是错的反馈，调整之前的行为，就这样不断的调整，算法能够学习到在什么样的情况下选择什么样的行为可以得到最好的结果。

就好比你有一只还没有训练好的小狗，每当它把屋子弄乱后，就减少美味食物的数量（惩罚），每次表现不错时，就加倍美味食物的数量（奖励），那么小狗最终会学到一个知识，就是把客厅弄乱是不好的行为。

两种学习方式都会学习出输入到输出的一个映射，监督式学习出的是之间的关系，可以告诉算法什么样的输入对应着什么样的输出，强化学习出的是给机器的反馈 reward function，即用来判断这个行为是好是坏。
另外强化学习的结果反馈有延时，有时候可能需要走了很多步以后才知道以前的某一步的选择是好还是坏，而监督学习做了比较坏的选择会立刻反馈给算法。

而且强化学习面对的输入总是在变化，每当算法做出一个行为，它影响下一次决策的输入，而监督学习的输入是独立同分布的。

通过强化学习，一个 agent 可以在探索和开发（exploration and exploitation）之间做权衡，并且选择一个最大的回报。 

exploration 会尝试很多不同的事情，看它们是否比以前尝试过的更好。 

exploitation 会尝试过去经验中最有效的行为。

一般的监督学习算法不考虑这种平衡，就只是是 exploitative。

### 强化学习和非监督式学习的区别：

非监督式不是学习输入到输出的映射，而是模式。例如在向用户推荐新闻文章的任务中，非监督式会找到用户先前已经阅读过类似的文章并向他们推荐其一，而强化学习将通过向用户先推荐少量的新闻，并不断获得来自用户的反馈，最后构建用户可能会喜欢的文章的“知识图”。

对非监督学习来说，它通过对没有概念标记的训练例进行学习，以发现训练例中隐藏的结构性知识。这里的训练例的概念标记是不知道的，因此训练样本的歧义性最高。对强化学习来说，它通过对没有概念标记、但与一个延迟奖赏或效用（可视为延迟的概念标记）相关联的训练例进行学习，以获得某种从状态到行动的映射。这里本来没有概念标记的概念，但延迟奖赏可被视为一种延迟概念标记，因此其训练样本的歧义性介于监督学习和非监督学习之间。

需要注意的是，监督学习和非监督学习从一开始就是相对的，而强化学习在提出时并没有从训练样本歧义性的角度考虑其与监督学习和非监督学习的区别，因此，一些早期的研究中把强化学习视为一种特殊的非监督学习。事实上，对强化学习的定位到目前仍然是有争议的，有的学者甚至认为它是与“从例子中学习”同一级别的概念。

从训练样本歧义性角度进行的分类体系，在近几年可望有一些扩展，例如多示例学习（multi-instancelearning）等从训练样本歧义性方面来看很特殊的新的学习框架有可能会进入该体系。但到目前为止，没有任何新的框架得到了公认的地位。另外，半监督学习（semi-supervisedlearning）也有一定希望，它的障碍是半监督学习中的歧义性并不是与生俱来的，而是人为的，即用户期望用未标记的样本来辅助对已标记样本的学习。这与监督学习、非监督学习、强化学习等天生的歧义性完全不同。半监督学习中人为的歧义性在解决工程问题上是需要的、有用的（对大量样本进行标记的代价可能是极为昂贵的），但可能不太会导致方法学或对学习问题视点的大的改变。

**强化学习和前二者的本质区别**:没有前两者具有的明确数据概念，它不知道结果，只有目标。数据概念就是大量的数据，有监督学习、无监督学习需要大量数据去训练优化你建立的模型，就像猫狗识别，用n多张猫狗图片去训练模型，经过训练优化后，你用一张崭新的猫狗图片让模型作出判断，这个模型就知道是猫还是狗。

## 10.4 强化学习主要有哪些算法？
强化学习不需要监督信号,可以在模型未知的环境中平衡探索和利用, 其主要算法有蒙特卡罗强化学习, 时间差分(temporal difference: TD)学习, 策略梯度等。典型的深度强化学习算法特点及性能比较如下图所示：

![](./img/ch10/10-4.png)
 
除了上述深度强化学习算法，还有深度迁移强化学习、分层深度强化学习、深度记忆强化学习以及多智能体强化学习等算法。
## 10.5深度迁移强化学习算法 
传统深度强化学习算法每次只能解决一种游戏任务, 无法在一次训练中完成多种任务. 迁移学习和强化学习的结合也是深度强化学习的一种主要思路。

Parisotto等提出了一种基于行为模拟的深度迁移强化学习算法. 该算法通过监督信号的指导, 使得单一的策略网络学习各自的策略, 并将知识迁移到新任务中. Rusa等提出策略蒸馏(policy distillation)深度迁移强化学习算法. 策略蒸馏算法中分为学习网络和指导网络, 通过这两个网络Q值的偏差来确定目标函数,引导学习网络逼近指导网络的值函数空间. 此后,Rusa等又提出了一种基于渐进神经网络(progressive neural networks, PNN)的深度迁移强化学习算法.PNN是一种把神经网络和神经网络连起来的算法. 它在一系列序列任务中, 通过渐进的方式来存储知识和提取特征, 完成了对知识的迁移. PNN最终实现多个独立任务的训练, 通过迁移加速学习过程, 避免灾难性遗忘. Fernando 等提出了路径网络(PathNet)[45].PathNet可以说是PNN的进阶版. PathNet把网络中每一层都看作一个模块, 把构建一个网络看成搭积木,也就是复用积木. 它跟PNN非常类似, 只是这里不再有列, 而是不同的路径. 	PathNet将智能体嵌入到神经网络中, 其中智能体的任务是为新任务发现网络中可以复用的部分. 智能体是网络之中的路径, 其决定了反向传播过程中被使用和更新的参数范围. 在一系列的Atari强化学习任务上, PathNet都实现了正迁移, 这表明PathNet在训练神经网络上具有通用性应用能力.PathNet也可以显著提高A3C算法超参数选择的鲁棒性. Schaul等提出了一种通用值函数逼近器(universalvalue function approximators, UVFAs)来泛化状态和目标空间．UVFAs可以将学习到的知识迁移到环境动态特性相同但目标不同的新任务中.
## 10.6分层深度强化学习算法 
分层强化学习可以将最终目标分解为多个子任务来学习层次化的策略, 并通过组合多个子任务的策略形成有效的全局策略. Kulkarni等提出了分层DQN(hierarchical deep Q-network, h--DQN) 算法. h--DQN基于时空抽象和内在激励分层, 通过在不同的时空尺度上设置子目标对值函数进行层次化处理. 顶层的值函数用于确定宏观决策, 底层的值函数用于确定具体行动．Krishnamurthy等在h--DQN的基础上提出了基于内部选择的分层深度强化学习算法. 该模型结合时空抽象和深度神经网络, 自动地完成子目标的学习, 避免了特定的内在激励和人工设定中间目标,加速了智能体的学习进程, 同时也增强了模型的泛化能力. Kulkarni等基于后续状态表示法提出了深度后续强化学习(deep successor reinforcement learning,DSRL)．DSRL通过阶段性地分解子目标和学习子目标策略, 增强了对未知状态空间的探索, 使得智能体更加适应那些存在延迟反馈的任务．Vezhnevets等受封建(feudal)强化学习算法的启发, 提出一种分层深度强化学习的架构FeUdal网络(FuNs)[49]. FuNs框架使用一个管理员模块和一个工人模块. 管理员模块在较低的时间分辨率下工作, 设置抽象目标并传递给工人模块去执行. FuNs框架创造了一个稳定的自然层次结构, 并且允许两个模块以互补的方式学习. 实验证明, FuNs有助于处理长期信用分配和记忆任务,在Atari视频游戏和迷宫游戏中都取得了不错的效果。
## 10.7深度记忆强化学习算法 
传统的深度强化学习模型不具备记忆、认知、推理等高层次的能力, 尤其是在面对状态部分可观察和延迟奖赏的情形时. Junhyuk等通过在传统的深度强化学习模型中加入外部的记忆网络部件和反馈控制机制, 提出反馈递归记忆Q网络(feedback recurrent memory Q-network, FRMQN)). FRMQN模型具备了一定的记忆与推理功能, 通过反馈控制机制,FRMQN整合过去存储的有价值的记忆和当前时刻的上下文状态, 评估动作值函数并做出决策. FRMQN初步模拟了人类的主动认知与推理能力, 并完成了一些高层次的认知任务. 在一些未经过训练的任务中,FRMQN模型表现出了很强的泛化能力．Blundell等设计出一种模型无关的情节控制算法(model-free episode control, MFEC). MFEC可以快速存储和回放状态转移序列, 并将回放的序列整合到结构化知识系统中, 使得智能体在面对一些复杂的决策任务时, 能快速达到人类玩家的水平．MFEC通过反向经验回放, 使智能体拥有初步的情节记忆. 实验表明, 基于MFEC算法的深度强化学习不仅可以在Atari游戏中学习到有效策略, 还可以处理一些三维场景的复杂任务. Pritzel等在MFEC的基础上进一步提出了神经情节控制(neural episodic control, NEC),有效提高了深度强化学习智能体的记忆能力和学习效率[53]. NEC能快速吸收新经验并依据新经验来采取行动. 价值函数包括价值函数渐变状态表示和价值函数快速更新估计两部分. 大量场景下的研究表明,NEC的学习速度明显快于目前最先进的通用深度强化学习智能体.
## 10.8 多智能体深度强化学习算法
在一些复杂场景中, 涉及到多智能体的感知决策问题, 这时需要将单一模型扩展为多个智能体之间相互合作、通信及竞争的多智能体深度强化学习系统.Foerster等提出了一种称为分布式深度递归Q网络(deep distributed recurrent Q-networks, DDRQN) 的模型, 解决了状态部分可观测状态下的多智能体通信与合作的挑战性难题[54]. 实验表明, 经过训练的DDRQN模型最终在多智能体之间达成了一致的通信协1536 控制理论与应用第34 卷议, 成功解决了经典的红蓝帽子问题.让智能体学会合作与竞争一直以来都是人工智能领域内的一项重要研究课题, 也是实现通用人工智能的必要条件. Lowe等提出了一种用于合作–竞争混合环境的多智能体actor-critic 算法(multi-agent deepdeterministic policy gradient, MADDPG)[55]. MADDPG对DDPG强化学习算法进行了延伸, 可实现多智能体的集中式学习和分布式执行, 让智能体学习彼此合作和竞争. 在多项测试任务中, MADDPG的表现都优于DDPG. 
## 10.9强化学习开源框架
谷歌TensorFlow Agents ---TensorFlow的加强版,它提供许多工具，通过强化学习可以实现各类智能应用程序的构建与训练。这个框架能够将OpoenAI Gym接口扩展至多个并行环境，并允许各代理立足TensorFlow之内实现以执行批量计算。其面向OpoenAI Gy环境的批量化接口可与TensorFlow实现全面集成，从而高效执行各类算法。该框架还结合有BatchPPO，一套经过优化的近端策略优化算法实现方案。其核心组件包括一个环境打包器，用于在外部过程中构建OpenAI Gym环境; 一套批量集成，用于实现TensorFlow图步并以强化学习运算的方式重置函数; 外加用于将TensorFlow图形批处理流程与强化学习算法纳入训练特内单一却步的组件。

Roboschool：Roboschool 提供开源软件以通过强化学习构建并训练机器人模拟。其有助于在同一环境当中对多个代理进行强化学习训练。通过多方训练机制，您可以训练同一代理分别作为两方玩家（因此能够自我对抗）、使用相同算法训练两套代理，或者设置两种算法进行彼此对抗。Roboschool由OpenAI开发完成，这一非营利性组织的背后赞助者包括Elon Musk、Sam Altman、Reid Hoffman以及Peter Thiel。其与OpenAI Gym相集成，后者是一套用于开发及评估强化学习算法的开源工具集。OpenAI Gym与TensorFlow、Theano以及其它多种深度学习库相兼容。OpenAI Gym当中包含用于数值计算、游戏以及物理引擎的相关代码。Roboschool基于Bullet物理引擎，这是一套开源许可物理库，并被其它多种仿真软件——例如Gazebo与Virtual Robot Experimentation Platform（简称V-REP）所广泛使用。其中包含多种强化学习算法，具体以怨报德 异步深度强化学习方法、Actor-Critic with Experience Replay、Actor- Critic using Kronecker-Factored Trust Region、深度确定性策略梯度、近端策略优化以及信任域策略优化等等。

Coach：英特尔公司的开源强化学习框架，可以对游戏、机器人以及其它基于代理的智能应用进行智能代理的建模、训练与评估。Coach 提供一套模块化沙箱、可复用组件以及用于组合新强化学习算法并在多种应用领域内训练新智能应用的Python API。该框架利用OpenAI Gym作为主工具，负责与不同强化学习环境进行交换。其还支持其它外部扩展，具体包括Roboschool、gym-extensions、PyBullet以及ViZDoom。Coach的环境打包器允许用户向其中添加自定义强化学习环境，从而解决其它学习问题。该框架能够在桌面计算机上高效训练强化学习代理，并利用多核CPU处理相关任务。其能够为一部分强化学习算法提供单线程与多线程实现能力，包括异步优势Actor-Critic、深度确定性策略梯度、近端策略优化、直接未来预测以及规范化优势函数。所有算法皆利用面向英特尔系统作出优化的TensorFLow完成，其中部分算法亦适用于英特尔的Neon深度学习框架。Coach 当中包含多种强化学习代理实现方案，具体包括从单线程实现到多线程实现的转换。其能够开发出支持单与多工作程序（同步或异步）强化学习实现方法的新代理。此外，其还支持连续与离散操作空间，以及视觉观察空间或仅包含原始测量指标的观察空间。
## 10.10深度强化学习算法小结
基于值函数概念的DQN及其相应的扩展算法在离散状态、离散动作的控制任务中已经表现了卓越的性能, 但是受限于值函数离散型输出的影响, 在连续型控制任务上显得捉襟见肘. 基于策略梯度概念的,以DDPG, TRPO等为代表的策略型深度强化学习算法则更适用于处理基于连续状态空间的连续动作的控制输出任务, 并且算法在稳定性和可靠性上具有一定的理论保证, 理论完备性较强. 采用actor-critic架构的A3C算法及其扩展算法, 相比于传统DQN算法, 这类算法的数据利用效率更高, 学习速率更快, 通用性、可扩展应用性更强, 达到的表现性能更优, 但算法的稳定性无法得到保证. 而其他的如深度迁移强化学习、分层深度强化学习、深度记忆强化学习和多智能体深度强化学习等算法都是现在的研究热点, 通过这些算法能应对更为复杂的场景问题、系统环境及控制任务, 是目前深度强化学习算法研究的前沿领域.

展望未来，人工智能开发者们需要尽可能掌握上述框架以及其中所使用的各类强化学习算法。此外，还需要强化自身对于多代理强化学习架构的理解，因为其中多种框架都大量利用前沿博弈论研究成果。最后，还需要熟悉深度强化学习知识。

## 10.11强化学习发展历史
Bellman: Dynamic Programming (1956).
Sutton: TD Algorithm (1988).
Watkins: Q-Learning (1992).
Rummery: SARAS Algorithm (1994).
…..
Kocsis:  Upper Confidence Bound Monte Carlo Tree Search (2006).
Silver:  Deterministic Policy Gradient (2014).
….. Deep Reinforcement Learning.
Maybe Artificial General Intelligence (2030), by Richard Sutton

## 10.12近端策略优化PPO算法
OpenAI刚刚通过自己的研究博客介绍了一种新的优化算法Proximal Policy Optimization（近端策略优化，PPO），它在每一步迭代中都会尝试计算新的策略，这样可以让损失函数最小化，同时还能保证与上一步迭代的策略间的偏差相对较小。信任区域更新的功能就可以通过这种目标函数得到实现，它与随机梯度下降兼容，而且移除了Kullback–Leibler惩罚项及它的自适应升级功能差，从而简化了算法。在测试中，PPO算法在连续控制任务中取得了最好的效果，而且在Atari游戏测试中的表现几乎与ACER持平；考虑到PPO的简便性，这样的结果真是令人惊喜万分。

## 10.13Unity3D用ML-Agent工具包训练游戏智能体
Unity Machine Learning Agents (ML-Agents) 是一款开源的 Unity 插件，使得我们得以在游戏环境和模拟环境中训练智能体agent。您可以使用reinforcement learning（强化学习）、imitation learning（模仿学习）、neuroevolution（神经进化）或其他机器学习方法， 通过简单易用的Python API进行控制，对 Agent 进行训练。我们还提供最先进算法的实现方式（基于 TensorFlow），让游戏开发者和业余爱好者能够轻松地 训练用于2D、3D和VR/AR 游戏的智能 agent。 这些经过训练的agent可用于多种目的，包括控制NPC行为（采用各种设置，例如多个agent和对抗）、对游戏内部版本进行自动化测试、以及评估不同游戏设计决策的预发布版本。ML-Agents对于游戏开发者和 AI 研究人员双方 都有利，因为它提供了一个集中的平台，使得我们得以在Unity的丰富环境中测试AI的最新进展， 并使结果为更多的研究者和游戏开发者所用。
在reinforcement learning（强化学习）技术中，所学习的行为称为policy，我们想学习的policy本质上是一个从每个可能的观测到该观测下最优动作的映射。请注意，通过运行模拟来学习policy的过程被称为训练阶段，而让NPC使用其学习到的 policy 玩游戏的过程被称为预测阶段。ML-Agents提供了所有必要工具，从而使我们得以利用Unity作为模拟引擎来学习Unity环境中不同对象的 policy。

## 10.14强化学习新突破—基于情境的好奇心模型
###1.传统强化学习算法应用成果及其局限性
DeepMind利用DQN算法来玩Atari游戏和用AlphaGoZero下围棋；OpenAI团队利用OpenAI-Five来打Dota2；Google教机器人手臂如何抓住新目标。这些都是利用强化学习来实现的，虽然这种“巧克力加大棒”的方法简单而通用，并且取得了如此大的成功，但是标准的强化学习算法在面对智能体反馈信息很少的环境中表现不佳，尤其是在现实世界中的环境。举例来说，试想要在迷宫中寻找出口，虽然我们会不断搜索，但是仍然无法知道出口在哪。如果每一步行动都得不到“巧克力”这类肯定，而只有“继续前进”这一指示，那么大家都无法判断自己是否在朝着正确的方向前进。在缺少奖励机制的情况下，能推动我们前进的只有“好奇心”。
传统的“好奇心”实现是通过自我监督加以预测的好奇心驱动型探索，即ICM方法。该方法会建立起一套环境动态的预测模型，并在模型未能做出良好预测时向智能体提供“惊喜”作为奖励，从而最大化奖励值。但是环境中造成惊喜的方式很多会导致无法预料的结果。除此之外，基于惊喜的好奇心模型会大肆放松自己“拖延症”的问题，即不再做出任何能够实际解决初始任务的行动。比如在迷宫中的智能体可能会被电视节目这样的惊喜吸引而停止搜索。

###2.基于情景记忆的好奇心模型
基于情景记忆的好奇心模型可以提供与好奇心类似的奖励，它不仅能探索环境，而且还能解决原始任务。只要将模型提供的奖励和环境中稀疏的反馈信息相结合，那么合并后的奖励不再稀疏，就可以使用标准强化学习算法从中学习，这种方法扩展了强化学习解决的任务集。
基于情景的好奇心模型：观察结果被添加到智能体的记忆中，奖励基于智能体当前的观察结果与记忆中最相似的结果的差异来计算的。智能体会因为看到记忆中尚不存在的观察结果而获得更多奖励。正是因为观察到记忆中不存在的情景促使智能体到达新的位置，从而避免原地兜圈子，并最终帮助其找到目标。

###3.基于“惊喜”和“情景”的好奇心模型的区别
基于“情景”记忆的好奇心模型不仅没有预测未来，还会遍历过去的记忆信息，判断是否看过当前的观察结果。因此，智能体不会被“即时满足感”所吸引而停止行动，它必须去探寻没见过的情景，从而获得更多的奖励。

## 10.15反向强化学习算法
###1.概念
反向增强学习也被称为模仿学习（Imitation Learning），学徒学习（Apprentice Learning），可以说它是来解决另一类增强学习问题的方法。
我们可以在已知Reward的情况下求Value Function，进而求出Policy，但是如果Reward不是那么容易得到呢？反向增强学习给了我们一个逆向思维，我们不通过Reward求Policy，反过来，通过Policy求Reward怎么样？
在家认真学习高手的动作要领，并铭记于心，最后回到泳池一试身手。实际上，这就是一个完整的从反向增强学习到增强学习的过程。首先，从Policy的采样出发，我们学习到了Reward——什么样的姿势是正确的，当然其他的姿势多半不正确，接下来就可以利用这些学习到的Reward进行尝试，再学习到Policy。这样一轮结束了么？当然不是，此时我们可以认为自己的策略不如视频上学习到的Policy采样好，那么我们就可以找出一种解释来说明两者的差距，而这个解释就是Reward。
###2.算法
首先随机生成一个Policy，它就是我们未来要得到的Policy。
通过比较“官方”的Policy采样和自己Policy采样的差别，学习得到第一版的Reward
利用这一版的Reward提高自己Policy的水平——可以理解为Policy Improvement
回到第二步，如果两者的差别已经不大，那么我们就可以停止学习
###3.总结
（1）反向强化学习主要用于求解环境回报的问题，为了求解回报，我们需要知道最优策略或者最优策略的交互序列
（2）为了学习状态回报，我们需要确保最优策略下的状态价值不小于其他策略
（3）为了使求解出的回报更加有意义，我们需要限定回报的上限，同时关注最优策略和次优策略在状态价值上的关系
（4）当需要基于最优序列样本学习策略时，我们可以结合反向强化学习和强化学习，共同提高回报函数的精确度和策略的效果。

## 10.16强化学习、监督学习、无监督学习的区别：（明杰-复旦）
有监督学习：
 Labeled数据，直接反馈，预测未知label数据
无监督学习：
 Unlabeled数据，无反馈，寻找数据隐藏结构
强化学习特点：
 无监督数据，只有奖励信号
 奖励信号不一定实时，大部分情况奖励信号滞后
 研究的非i.i.d数据，时间序列
 当前的行为影响后续数据分布
总结：强化学习是逻辑推理，监督学习和无监督学习是死记硬背


